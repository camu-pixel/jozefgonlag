<!DOCTYPE html>
<html style="font-size: 16px;"><head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta charset="utf-8">
    <meta name="keywords" content=""><meta name="description" content="">
    <title></title>
    <link rel="stylesheet" href="/nicepage.css" media="screen">
    <script class="u-script" type="text/javascript" src="np://app.desktop.nicepage.com/jquery.js" defer=""></script>
    <script class="u-script" type="text/javascript" src="np://app.desktop.nicepage.com/nicepage.js" defer=""></script>
</head>
<body class="u-body u-xl-mode" data-style="blank" data-posts="" data-global-section-properties="{&quot;colorings&quot;:{&quot;light&quot;:[&quot;clean&quot;],&quot;colored&quot;:[&quot;clean&quot;],&quot;dark&quot;:[&quot;dark&quot;]}}" data-source="blank" data-lang="en" data-page-sections-style="[]" data-page-coloring-types="{&quot;light&quot;:[&quot;clean&quot;],&quot;colored&quot;:[&quot;clean&quot;],&quot;dark&quot;:[&quot;dark&quot;]}" data-page-category="&quot;Basic&quot;" data-back-link="https://nicepage.com/website-design" data-back-link-title="Free Website Design" data-wp-back-link="https://nicepage.com/wordpress-themes" data-wp-back-link-title="WordPress Theme" data-jm-back-link="https://nicepage.com/joomla-templates" data-jm-back-link-title="Joomla Templates" data-created-with-link="https://nicepage.com/wysiwyg-html-editor" data-created-with-title="WYSIWYG HTML Editor" data-wp-created-with-link="https://nicepage.com/wordpress-website-builder" data-wp-created-with-title="WordPress Page Builder" data-jm-created-with-link="https://nicepage.com/joomla-page-builder" data-jm-created-with-title="Joomla Website Builder"><section class="u-align-center u-clearfix u-block-c984-1" custom-posts-hash="T" data-post-id="2502854279" data-section-properties="{&quot;width&quot;:&quot;sheet&quot;,&quot;margin&quot;:&quot;none&quot;,&quot;stretch&quot;:true}" data-id="c984" data-posts-content="[{'images':[],'maps':[],'videos':[],'icons':[],'textWidth':1140,'textHeight':177,'id':1,'headingProp':'h1','textProp':'text'}]" data-style="text-heading-only" id="sec-9cc3" data-source="Sketch">
  <div class="u-clearfix u-sheet u-block-c984-2">
    
  <div class="u-align-center u-clearfix u-group-elements u-block-c984-5">
<div class="u-absolute-hcenter u-border-3 u-border-white u-expanded-width u-line u-line-horizontal u-block-c984-6"></div><h1 class="u-align-center u-text u-text-default u-block-c984-7">Research</h1><div class="u-border-3 u-border-white u-expanded-width u-line u-line-horizontal u-block-c984-8"></div></div></div>
  <style data-mode="XL" data-visited="true">@media (min-width: 1200px) {
  .u-block-c984-2 {
    min-height: 177px;
  }
  .u-block-c984-5 {
    width: 912px;
    min-height: 114px;
    margin-top: 45px;
    margin-left: auto;
    margin-right: auto;
    margin-bottom: 60px;
  }
  .u-block-c984-6 {
    position: absolute;
    top: 0;
    height: 3px;
    left: 0;
  }
  .u-block-c984-7 {
    font-weight: 900;
    margin-top: 28px;
    margin-left: auto;
    margin-right: auto;
    margin-bottom: 0;
  }
  .u-block-c984-8 {
    margin-top: 28px;
    height: 3px;
    margin-bottom: 0;
  }
}</style>
  <style data-mode="LG">@media (max-width: 1199px) and (min-width: 992px) {
  .u-block-c984-2 {
    min-height: 177px;
  }
  .u-block-c984-5 {
    width: 912px;
    margin-top: 45px;
    margin-right: auto;
    margin-bottom: 60px;
    margin-left: auto;
    min-height: 114px;
  }
  .u-block-c984-6 {
    position: absolute;
    height: 3px;
  }
  .u-block-c984-7 {
    font-weight: 900;
    margin-top: 28px;
    margin-right: auto;
    margin-bottom: 0;
    margin-left: auto;
  }
  .u-block-c984-8 {
    margin-top: 28px;
    height: 3px;
    margin-bottom: 0;
  }
}</style>
  <style data-mode="MD">@media (max-width: 991px) and (min-width: 768px) {
  .u-block-c984-2 {
    min-height: 177px;
  }
  .u-block-c984-5 {
    width: 720px;
    margin-top: 45px;
    margin-right: auto;
    margin-bottom: 60px;
    margin-left: auto;
    min-height: 114px;
  }
  .u-block-c984-6 {
    position: absolute;
    height: 3px;
  }
  .u-block-c984-7 {
    font-weight: 900;
    margin-top: 28px;
    margin-right: auto;
    margin-bottom: 0;
    margin-left: auto;
  }
  .u-block-c984-8 {
    margin-top: 28px;
    height: 3px;
    margin-bottom: 0;
  }
}</style>
  <style data-mode="SM">@media (max-width: 767px) and (min-width: 576px) {
  .u-block-c984-2 {
    min-height: 177px;
  }
  .u-block-c984-5 {
    width: 540px;
    margin-top: 45px;
    margin-right: auto;
    margin-bottom: 60px;
    margin-left: auto;
    min-height: 114px;
  }
  .u-block-c984-6 {
    position: absolute;
    height: 3px;
  }
  .u-block-c984-7 {
    font-weight: 900;
    margin-top: 28px;
    margin-right: auto;
    margin-bottom: 0;
    margin-left: auto;
  }
  .u-block-c984-8 {
    margin-top: 28px;
    height: 3px;
    margin-bottom: 0;
  }
}</style>
  <style data-mode="XS">@media (max-width: 575px) {
  .u-block-c984-2 {
    min-height: 177px;
  }
  .u-block-c984-5 {
    width: 340px;
    margin-top: 45px;
    margin-right: auto;
    margin-bottom: 60px;
    margin-left: auto;
    min-height: 114px;
  }
  .u-block-c984-6 {
    position: absolute;
    height: 3px;
  }
  .u-block-c984-7 {
    font-weight: 900;
    margin-top: 28px;
    margin-right: auto;
    margin-bottom: 0;
    margin-left: auto;
  }
  .u-block-c984-8 {
    margin-top: 28px;
    height: 3px;
    margin-bottom: 0;
  }
}</style>
</section><section class="u-align-center u-clearfix u-block-0214-1" custom-posts-hash="T" data-post-id="3263379132" data-section-properties="{&quot;margin&quot;:&quot;none&quot;,&quot;stretch&quot;:true}" data-id="0214" data-posts-content="[{'images':[],'maps':[],'videos':[],'icons':[],'textWidth':1140,'textHeight':354,'id':1,'headingProp':'h1','textProp':'text'}]" data-style="headline-h4-h2-text" id="sec-304d" data-source="Sketch">
  <div class="u-clearfix u-sheet u-block-0214-2">
    
    
    <p class="u-text u-block-0214-5"><span style="font-weight: 700; font-size: 1.875rem;">An investigation into using non-verbal auditory cues over visual and&nbsp;</span><br><span style="font-weight: 700; font-size: 1.875rem;">verbal communication in Virtual Environments.&nbsp;</span><br><br>Jozef Gonlag - 000887040
<br>
<br>Supervisor: Damon Daylamani-Zad
<br>
<br>COMP1682 Final Year Project
<br>
<br>Creative Digital Media
<br>
<br>Due date: 24th April 2018
<br>
<br>Word count: 8151
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>Abstract:
<br>
<br>Due  to emerging technologies becoming more immersive, the need for a  well-designed virtual environment grows with it. This investigation aims  to measure the usability and effectiveness of using auditory cues  instead of visual, verbal and non-verbal cues in virtual worlds and how  they can be used appropriately to reduce a user’s cognitive load.
<br>
<br>
<br>
<br>
<br>
<br>Table of Contents
<br>1. Introduction.
<br>
<br>1.2 Aims and objectives
<br>
<br>1.3 Approach
<br>
<br>1.4 Identifying the problem/situation
<br>
<br>1.4.1 Immersive technologies
<br>
<br>2. Literate research/review
<br>
<br>2.1 Non-verbal communication in Virtual Worlds
<br>
<br>2.2 Non-verbal communication in Virtual Reality
<br>
<br>2.3 Virtual Worlds(VWs) and Virtual Reality(VR) embodiments
<br>
<br>2.4 Basics of Sound Design
<br>
<br>2.5 Advantages of spatial sound
<br>
<br>2.6 Cognitive psychology
<br>
<br>2.7 Attention and Distraction
<br>
<br>2.8 Metaphors
<br>
<br>2.8.1 Auditory icons
<br>
<br>2.9 Affordances
<br>
<br>2.9.1 Auditory affordances
<br>
<br>3. Similar User Studies
<br>
<br>3.1 Auditory Cues for Spatial Cognitive Behaviours
<br>
<br>3.2 Auditory versus Visual interfaces while driving
<br>
<br>3.3 Sense of presence
<br>
<br>4. Virtual Testing Environment Design
<br>
<br>4.1 The first task
<br>
<br>4.2 The second task
<br>
<br>4.3 The third task
<br>
<br>4.4 Design progression
<br>
<br>4.5 Testing
<br>
<br>4.6 Evaluation of test results
<br>
<br>4.7 Summary
<br>
<br>4.7.1 Difference in participants experience with computer systems
<br>
<br>5. Conclusion
<br>
<br>5.1 Reflection And Expanding The Investigation
<br>
<br>6. References
<br>
<br>7. Appendices
<br>
<br>Appendix A – System Usability Scale Questionnaire
<br>
<br>Appendix B – Full Questionnaire data<br><br>
<br><br><br><br><br>1. Introduction
<br>At  the time of this investigation the virtual reality (VR) technology and  designing interactions for it are in a state of expansion. Although  virtual reality technology has been around for a very long time, the  current state of it has only been a widely available consumer product  since 2015. Stereoscopy was used in the 1830’s by Charles Wheatstone and  later popularised by Oliver Wendell Holmes in the late 1800’s and early  1900’s in the form of glasses that would create the illusion of depth  (Holmes, 1859).
<br>
<br>Audio  would not be used in combination with virtual worlds until the 1960’s  in what Morton Heilig would call the “Sensorama”. A simulation device  that would provide the illusion of 3D images, including stereo sound,  vibrations, smell and wind. 
<br>
<br>Now  in the 2010’s immersive technologies have reached near-complete levels  of immersion, with head-mounted displays that track one’s head motions,  sensory hardware tracking one’s hand and feet gestures, and  omni-directional treadmills creating the possibility of physically  walking in a Virtual Reality environment. Looking at the history of  immersive technologies it is safe to say that the importance of auditory  communications in virtual worlds would increase in conjunction with the  advancement of the technologies over the years.
<br>
<br>This  document will focus on investigating the effectiveness and usability of  auditory cues instead of visual, verbal and non-verbal communications  in virtual worlds.
<br>
<br>The  difference between verbal and non-verbal communication is that verbal  communication always uses words to convey information, whereas  non-verbal communication uses everything else, including but not limited  to gestures, expressions, and auditory cues.
<br>
<br>The  difference between auditory and visual cues is that visual cues  primarily convey information, in the sense of what this information  contains. Whereas auditory cues primarily convey information about a  different piece of information, e.g. its location in 3D space or the  intensity of the information.
<br>
<br>Due  to most virtual worlds being displayed on screens that are in front of  the user, and the typical audio setup an average user uses being limited  to stereo audio, the user can never differentiate more between whether  audio is coming from left, right, up or down within the visual  limitation of the screen. This is one of the reasons why spatial sound  has become important for immersive worlds.
<br>
<br>Although  there is a large difference between visual and auditory communication,  both are required to create a well-functioning interactive virtual  world. During the design process of an interactive virtual world both of  these types of communication are considered important to convey  instructive information to the user. Although humans depend on their  visual sense for 80% of external information, the leftover 20% is still  relevant enough to investigate, to try and see how effective the  dependency of humans on auditory senses is (Takao et al., 2002).
<br>
<br>1.2 Aims and objectives
<br>
<br>The  aim of this project is to investigate the usability and effectiveness  of using auditory cues instead of visual, verbal and nonverbal  communications in a virtual reality environment.
<br>
<br>The objectives that are necessary to reach this aim are:
<br>
<br>- Collect literature, research documents and journals related to auditory interactions in virtual environments
<br>
<br>- Analyse collected literature
<br>
<br>- Design a testing prototype based on research
<br>
<br>- Create a prototype
<br>
<br>- Validate prototype using experiments
<br>
<br>- Evaluate results
<br>
<br>
<br>
<br>1.3 Approach
<br>
<br>To  reach the aims and objectives of this investigation a virtual test  environment will be created in which the user will have to perform  multiple interaction tasks.
<br>
<br>The design of this virtual test environment, and primarily the content of the tasks, will be based on the literature research.
<br>
<br>The  user will be timed and questioned afterwards about the difficulty and  usability of the tasks. After all participants have performed the tasks,  the data provided by the questionnaires will be made into statistics  which will then be analysed to form a conclusion on the subject.
<br>
<br>The  System Usability Scale, or S.U.S., is a widely used questionnaire for  measuring perceptions of usability and is considered an industry  standard to measure a systems usability (Sauro, 2011). After the user  completes a task the user will be prompted to answer 9 questions about  the usability and interactions of the task. Once the 9 questions have  been answered the participant will carry on to the next task until all  three tasks have been completed this way. The S.U.S. can be viewed in  Appendix A.
<br>
<br>The  data that the results of the questionnaire will provide be used to  determine a conclusion on how effectively auditory cues can function  instead of visual, verbal and nonverbal cues in a virtual world.
<br>
<br>1.4 Identifying the problem/situation
<br>
<br>With  virtual and augmented reality becoming a more commonly used technology  over the past 2 years, the need for a well-designed user experience  grows with it. The first quarter of 2016 brought the release of 3 major  companies releasing their commercial VR-headsets, causing the  technology’s market to grow over 1000% from 2015 to 2016 (Smith, 2016).  With these VR products finally becoming widely available to the public a  lot of user feedback could be collected about the experiences using  these devices. This includes the current issues with usability such as  motion sickness and disorientation, but also the user decision making  and interaction design. The current problem with designing interactions  for Virtual Reality is that the technology is extremely new and therefor  is still experimental in certain areas. For companies who want to sell  this technology this creates risks such as bad first impressions to a  company’s product, or the technology as a whole.
<br>
<br>
<br>
<br>1.4.1 Immersive technologies
<br>
<br>The most prominent and uprising technology when it comes to immersive technologies are Virtual Reality headsets.
<br>
<br>Figure 1. (Worldwide virtual reality headset unit sales by brand in 2016 and 2017, 2018)
<br>Figure 1. (Worldwide virtual reality headset unit sales by brand in 2016 and 2017, 2018)
<br>
<br>This  bar graph shows the sales of the top 6 most purchased VR Headsets on  the current market. Sales of the phone-based headsets such as Google  Cardboard(Google LLC, 2018a), Samsung Gear VR(Samsung Electronics, 2018)  and the Google Daydream(Google LLC, 2018b)  much surpass the full VR  headsets due to their much lower costs. Phone-based VR headsets range  between £5 to £100, while the full headsets such as Sony PlayStation  VR(Sony Interactive Entertainment, 2018), HTC Vive(HTC Corporation,  2018) and Oculus Rift(Oculus VR, 2018) cost between £300 and £600. Not  included in this graph is the Mixed Reality headset HoloLens (Microsoft,  2018), which is available for either £2700 for the Development edition  or £4500 for the Commercial Suite edition. Although commercially  available, it is not yet within the price range that consumers are  willing to pay to have this technology in their homes.
<br>
<br>Taking  the immersion of audio into consideration, the fully featured headsets  stand out due to their built-in headphones. Immersion is generally much  lower on the phone-based headsets due to the lack of controls and lack  of built-in headphones. Graphically the fully featured headsets are also  superior to phone-based headsets because the fully featured headsets  either connect to a powerful gaming console 4 or a desktop computer on  which the actual VR software runs.
<br>
<br>
<br>
<br>
<br>
<br>
<br>2. Literate research/review
<br>2.1 Non-verbal communication in Virtual Worlds
<br>
<br>According  Argyle (Argyle, 1972), there are three types of non-verbal  communication in human social interactions. The first one being one of  attitude, emotions and the manipulation of the immediate social  situation. The second one being to complement and support verbal  communication. And the third one as a replacement for verbal  communication.
<br>
<br>In  the most popular type of virtual worlds, which are Massive Multiplayer  Online Roleplaying games(MMORPG’s), the use of non-verbal communications  between players is limited to the rules of the game and depends on the  actions and possibilities a player is able to do with their in-game  avatar. Often MMORPG’s will allow their players to express themselves  through the use of emotes. Emotes will allow the player to express  certain emotions and actions through their avatar, such as waving to  another player, dancing, laughing and many more depending on the game.
<br>
<br>Figure2.PNG
<br>Figure 2. The Emote window in an MMORPG (NCSOFT Corp., 2018).
<br>
<br>But  non-verbal communications between players is not the only type of  non-verbal communications in such virtual world. The world the player is  in will also convey information to the player such as footpaths leading  you to different locations or different signage throughout the world.  Aspects of the Graphical User Interface (GUI) of the game are also part  of non-verbal communications, specifically those that use icons instead  of text.
<br>
<br>Figure3.PNG
<br>Figure 3. An example of a GUI using icons in an MMORPG (CipSoft GmbH, 2018).
<br>
<br>To  identify a few of the icons used in the previous image, they represent  opening or closing certain windows or avatar behaviour, such as opening  the Skill or Battle window, or setting your character to automatically  follow a target or not.
<br>
<br>Another  big difference in communications between the real world and a virtual  world is that social interactions in virtual worlds are more intense  than those in the physical world due to the fact that participants do  not require to concentrate on maintaining socially acceptable non-verbal  gestures in face-to-face interactions and can thereby focus on the  content of the message itself, resulting in a more articulated and  personal message.(Brown et al., 1999) This suggests that the way of  communications in virtual worlds cause to filter out nonverbal “noise”  from conversations.
<br>
<br>
<br>
<br>2.2 Non-verbal communication in Virtual Reality
<br>
<br>In  Virtual Reality there is a lower need to design and implement digital  ways of non-verbal communications between players due to the possibility  of using body language through the use of the motion sensors of the  Virtual Reality headset, and optional hand and feet tracking sensors.
<br>
<br>With  the current state of Virtual Reality being relatively new compared to  those of virtual worlds, not many worlds such as MMORPG’s have been  developed for it. At the time of this investigation there is only one  available MMORPG, OrbusVR (Ad Alternum, 2018), with others still in  development. Online virtual reality environments that are based around  multiplayer are more available, but they are primarily focused on single  activities such as solving a puzzle or purely social interactions.
<br>
<br>One  social Virtual Reality application that has recently risen in  popularity is “VRchat”(VRChat Inc., 2018). VRchat is a free game  available for the Steam(Valve Corporation, 2018) platform in which users  can socialise with each other through the use of Virtual Headsets,  which they use to control their avatar in a 3D online world. Depending  on the individual hardware set up of the user, the players can  non-verbally communicate with each other the same way one can in the  real world. Yet VR Chat allows for a similar way of, allowing to let  people’s avatars to perform tricks such as backflips, or the more simple  waving and nodding gestures.
<br>
<br>Figure4.PNG
<br>Figure 4. The Emote window in VR. Chat
<br>
<br>Since  one of the core mechanics of an MMORPG’s is playing together with many  other players within the same world, and the user base of Virtual  Reality headsets that are capable of running such video games is still  low compared to MMORPG’s in non VR worlds, the development of such games  in VR have not been extensive.
<br>
<br>Bailenson  et al (Bailenson et al., 2002) conducted a series of experiments in  which participants were placed in a virtual space by wearing a  head-mounted display and asked to inspect the details of three different  virtual objects. The first object being a photorealistic person, the  second a flat two-dimensional person, and third a non-human pillar.   Results showed that the more realistic the object the more personal  space is considered by the participants. This means that the more  realistic the virtual environment is, the more realistic and more  naturally the behaviour of the participants.
<br>
<br>
<br>
<br>
<br>2.3 Virtual Worlds(VWs) and Virtual Reality(VR) embodiments
<br>
<br>Although VWs and VR have a lot in common, they are separated by their assumptions about embodiment and proprioception.
<br>
<br>Embodiment can be defined in several different ways (Rohrer, 2007). The two most relevant to VWs and VR are:
<br>
<br>Phenomenology: This describes the human’s conscious awareness of our own bodies and their role in mediating our experience of the world.
<br>
<br>Perspective: The term embodiment can be used to refer to a particular point-of-view from which the embodiment’s perspective is taken.
<br>
<br>In Virtual Reality the user’s sense of being embodied in the experience is overlapped with the user’s physical embodiment.
<br>
<br>In  Virtual Worlds embodiment is more about the user’s ability to imagine  his or her perspective into the virtual space through an avatar.
<br>
<br>2.4 Basics of Sound Design
<br>
<br>Within any environment in which sound is designed, there can be two types of sounds. Diegetic and non-diegetic sounds.
<br>
<br>Diegetic  sounds are sounds that originate directly from an object of the  observable environment. This can be from a visible sound source or from a  source that is implied to be within the environment. Examples of  diegetic sounds are voices of characters, sounds made by objects within  the environment, music coming from an instrument within the environment.
<br>
<br>Non-diegetic  sounds come from a source that is neither visible nor implied to be  within the environment. Examples of non-diegetic sounds are narrator’s  commentary, sound effects to complement actions that happen in the  environment, and background music.
<br>
<br>It  is possible to break these rules by transitioning, for example,  background music into music that is coming from a radio within the  environment, an effect that is sometimes used in film design.
<br>
<br>Because  in a virtual world the user has control over the camera, a sound that  is off-screen can be brought to onscreen by the user, although this  doesn’t change a sound from diegetic to non-diegetic as the main  difference is whether it’s source is implied to be in the world or not.
<br>
<br>
<br>
<br>
<br>
<br>2.5 Advantages of spatial sound
<br>
<br>Humans  depend on their visual sense for 80% of external information, therefore  the visual sense is most natural and important during work with  displays (Takao et al., 2002). However, making more use of the human's  auditory senses is thought to be worthwhile for several reasons. In  particular the auditory sense can receive information from any direction  compared to the visual one. This is the main advantage that auditory  cues have over visual cues. This means that audible information can be  transferred and conveyed from any direction in a 3D space, compared to  the limited visual range (Figure 5).
<br>
<br>Figure5.PNG
<br>Figure 5. Comparison of transferrable ranges for information (Takao et al., 2002).
<br>
<br>A  3D Sound system can generate the origin of a sound from any direction  around the listener, while a 2D sound system can only generate this in  limited directions in front of the listener (Figure 6).
<br>
<br>Figure6.PNG
<br>Figure 6. 3D Sound vs 2D Sound (Takao et al., 2002).
<br>
<br>Spatial  audio interfaces represent audio items in different spatial locations  in a 3D space, this allows them to convey additional information such as  the direction and distance from the listener (Sodnik et al., 2008).  Because human ears are located at either side of the head, the so called  binaural effect enable the human to better determine the direction and  location of a sound source. Spatial sound can be very effectively used  through headphones.
<br>
<br>Takao  et al (Takao et al., 2002) conducted a research into an Acoustic User  Interface, in which a test environment is designed with the purpose of  reducing the cognitive load on the visual senses while driving. In  combination with a voice recognition system the driver can perform and  confirm tasks by using his or her voice and ears, such as getting  directions from a GPS system. The same concept can be used while  designing virtual reality environments, where the virtual space  surrounds the user. The visual range of a human cannot be extended  without breaking the immersion of the system (e.g. by expanding the  field of view to unnatural levels), this is why including auditory cues  in a virtual reality environment is crucial to convey information that  is outside the user’s visual range without overloading the cognitive  load of a user.
<br>
<br>
<br>
<br>2.6 Cognitive psychology
<br>
<br>Cognition  is defined as the mental action or process of acquiring knowledge and  understanding through thought, experience, and the senses, by the oxford  dictionary (Oxford Dictionary, 2018)
<br>
<br>In  the scenario of interacting with a virtual environment, the interaction  with the environment is cognitive. Cognition is how we understand and  therefor interact with this environment.
<br>
<br>Kahneman  (Kahneman, 2010) describes in his book “Thinking, Fast and Slow” the  two different types of cognition as Fast thinking and Slow thinking.  While according to Norman (Norman, 2013), in his book “The Design of  Everyday Things”,  there are three types of these mental processing.  Visceral, behavioural and reflective.
<br>
<br>Visceral  processing consists of the most basic emotions such as those that  determine something as good or bad and safe or dangerous in a matter of  milliseconds. It is our natural instinct and is very similar for very  person.
<br>
<br>Behavioural processing primarily consists of learned skills and skills that are based off experiences.
<br>
<br>Finally,  the reflective level of processing is the most conscious type of  processing and consists mostly of conscious thoughts and decision  making.
<br>
<br>The  cognitive processes that are most relevant to interacting with a  virtual world are attention, perception, and memory. Paying attention to  something is the process of selecting things to concentrate on from the  available possibilities (Sharp, Preece and Rogers, 2015). Perception  refers to how information is acquired from the environment the receiver  of the information is in, through the senses of the receiver, and  transformed into a cognitive experience (Rohrer, 2007).  Memory is the  ability to recall various kinds of knowledge within our mind, and within  virtual interaction it plays a large role in the use of metaphors  (Sharp, Preece and Rogers, 2015).
<br>
<br>The  types of mental processing most relevant to interactive virtual worlds  are Behavioural and Reflective. Although depending on the type of  virtual environment, Visceral processing can play a larger role e.g. in  Virtual Reality environments than it does in a regular virtual world due  to the world around the user being experienced as more natural which  allows for more natural instincts to kick in.
<br>
<br>One  of the ways to avoid overloading the cognitive senses of the user is by  having the least amount of distractions within the virtual environment.  Important cues within the virtual world should not be interrupted or  overshadowed by other cues to avoid overlap between different types of  cues.
<br>
<br>2.7 Attention and Distraction
<br>
<br>Attention  is defined as “The mental faculty of considering or taking notice of  someone or something” by the Oxford dictionary (Oxford Dictionary,  2018).
<br>
<br>Distraction  is defined as “A thing that prevents someone from concentrating on  something else.” by the Oxford dictionary (Oxford Dictionary, 2018).
<br>
<br>In  the scenario of this research any distraction within the interactions  would lead to a reduced amount of attention on the task.
<br>
<br>When  designing a virtual environment, it is important to consider that one  piece of information can distract the attention away from another piece  of information when used at the same time. Although the opposite can  also be true. According to Bahrick (Bahrick, Lickliter and Flom, 2004)  cues that stimulate multiple senses at the same time will trigger the  brain to put these pieces of information together as one, but only if  elements of those cues are similar to each other. An example of this is  both the visual and auditory information of a bouncing ball originate  from the same location. Another example is the element of rhythm of  clapping hands is conveyed both visually and auditory. If, in a  recreation of these scenarios in a virtual world, these elements would  be asynchronous between audio and visuals, the user would not as easily  assign the sounds with each other and could conclude the sounds might be  associated to an offscreen object instead.
<br>
<br>When  humans are in their infant years they learn these natural associations  through experiences, e.g. that the sound of footsteps corresponds to  walking feet, the sound of a voice corresponds to a face and a sharp  crashing sound corresponds to breaking glass.
<br>
<br>To  keep the attention of a user in the place where you want it to be as a  designer of the virtual world, the use of audio and auditory cues plays a  large role. If the environment is very instruction heavy it is  important to keep the auditory cues consistent in both types of audio  and volume. Alternatively, audio can play a large role in getting the  user’s attention extremely abruptly if this is required. Examples of  this are giving the user a warning of quickly incoming objects with loud  alarms or it can be effectively used to scare a user if this is part of  the design of the environment e.g. in action and/or horror video games.
<br>
<br>
<br>
<br>
<br>2.8 Metaphors
<br>
<br>Metaphors  are defined as a figure of speech in which a word or phrase is applied  to an object or action to which it is not literally applicable by the  Oxford dictionary (Oxford Dictionary, 2018). And it is in this way that  they can also be applied to virtual objects to instruct a user in the  most natural way.
<br>
<br>It  is another aspect of interaction design that benefits from natural  associations. In the design of Graphical User Interfaces metaphors are  commonly used to avoid using verbal communications. Instead of labelling  a button with a word, an icon that represents the action of the button  is placed on it. But it isn’t always possible to represent an action  with an naturally relating icon, and this is where metaphors are used.  Some examples of metaphors in graphical user interfaces are using an  icon of scissors for the cutting of selected text or files, a magnifying  glass being used for a search button and a bell representing an  activated alarm.
<br>
<br>Using metaphors in the auditory form can also be described as using auditory icons or “Earcons”.
<br>
<br>
<br>
<br>
<br>2.8.1 Auditory icons
<br>
<br>The  concept of using natural everyday sounds to represent events and object  in a computer interface was first developed by William W. Gaver in 1986  (Gaver, 1986). According to Gaver auditory icons, also called  “earcons”, can be divided into three categories based on their  abstraction level, iconic, metaphorical or symbolic. An iconic auditory  icon tries to reproduce an event as realistically as possible, while a  metaphorical auditory icon tries to establish an analogy between the  event and the associated sound. An auditory icon can not only represent a  single item but can also represent its position in a hierarchical  structure in audio-only interfaces such as a telephone menu based  interface. It has been shown that auditory icons can successfully  improve the usability of interfaces for mobile use where the visual  senses are already heavily occupied.
<br>
<br>Depending  on the sound, it is extremely easy for a user to interpret what a  simple auditory icon represents, e.g. the sound of a rattling trash bin  representing the deletion of a file, or the sound of an alarm  representing a negative message that plays the same time an error  message pops up (Sodnik et al., 2008).
<br>
<br>
<br>
<br>
<br>
<br>
<br>2.9 Affordances
<br>
<br>Another aspect of attention and recognition in virtual worlds are affordances.
<br>
<br>The  affordance of an object is the aspect of said object that tells the  user the purpose of that object. An example of an affordance in the real  world could be a door handle. The shape and position of a door handle  tells a person how to grab it and in which direction to push it. The  shape of a standard horizontal door handle will prompt a person to press  it down with one hand, while a vertical door handle prompts a person to  pull the door. A mistake that is still often made is one directional  doors having vertical handles on both sides of the door, implying that  one can pull the door from both sides while it is only possible from one  of the sides. To be able to push a door open, the handle would not be  necessary and is therefore redundant and possible even confusing. This  is also one of the reason why doors in hospitals always open both ways  and never have handles to avoid any problems.
<br>
<br>In  a graphical user interface, the affordance of a button is the fact that  it tells the user that it can be pressed by having the visual  representation of a button and by clearly having a different visual  representation compared to everything else on the GUI.
<br>
<br>Shin  (Shin, 2017) conducted a case study that looks at the role of  affordances in a virtual reality and how they influence the acceptance  of an educational virtual reality learning environment. Virtual Reality  is a heuristic tool that can be used for things such as sharing  information and data visualization. This also allows it to be a great  way of teaching a user about a topic or to share data. Virtual Reality  learning environments allow users to conduct experiments with no risk to  themselves or others that would otherwise not be possible in actual  reality.
<br>
<br>Although  Virtual reality can be a very successful educational tool, the current  state of the usability of Virtual Reality has been claimed to have a  negative effect on the user acceptance of Virtual Reality (Fetscherin  and Lattemann, 2008).
<br>
<br>This  is due to the fact that new users of the technology find may find the  immersion levels of Virtual Reality overwhelming compared to a regular  Personal Computer setup with a screen, mouse and keyboard. Combine this  with the limitations of sensory hardware and graphical limitations and  it is very plausible that even though the user is fully surrounded by a  3D environment, he or she is not fully immersed into the world.
<br>
<br>Despite  VR becoming more available to consumers and its graphics becoming more  realistic, a lot of questions about how users of VR feel about the  technology remain unanswered. Although according to Dalgarno (Dalgarno  and Lee, 2010), a user’s engagement with a Virtual reality learning  environment is much more active rather than passive. This is because a  Virtual Reality learning environment can be completely designed with  teaching in mind, therefore the environment can be completely free of  distractions of the content of the virtual world.
<br>
<br>Subjectivity,  perception, role and context play a critical role in the advancement of  future VR systems for educational purposes.
<br>
<br>
<br>
<br>2.9.1 Auditory affordances
<br>
<br>Describing  and explaining auditory affordances objectively can be difficult due to  the perception of them primarily being a subjective mental phenomenon  (Steenson and Rodger, 2015).
<br>
<br>To  describe auditory affordances in a psychological way, they are the  transformation of objective input into subjective output. A piece of  audio is by itself objective, but once perceived by a listener the  listener will receive it as subjective.
<br>
<br>Auditory  cues have the positive benefit of having lower cognitive load than  visual ones as it does not require the receiver to redirect their eyes  and/or head to fully interact with it (Baldis, 2001).
<br>
<br>Recognising auditory affordances would classify under the visceral and behavioural processing of interactions.
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>3. Similar User Studies
<br>Within  the reviewed literature, several technologies and user studies looked  into similar investigations as this one. This chapter will cover  multiple technologies and user studies in more detail and explain how  they relate to using auditory cues in virtual worlds.
<br>
<br>
<br>
<br>3.1 Auditory Cues for Spatial Cognitive Behaviours
<br>
<br>This  research focuses on the connection between music and how it affects the  spatial cognitive process and performances (Jeong and Yi, 2015).
<br>
<br>While  playing video games people process various cognitive decisions  continuously based on several sensory stimuli that the game feeds to the  user. Not only visual information delivers an immersive and fulfilling  experience, but also auditory stimuli. Moreover, auditory cues might  help reduce gamer’s learning time on tutorial processes. Auditory  information can be enough to recognise 3D spaces in spatial  information-bases games such as first person and third person shooters.
<br>
<br>In  cases of movement-based games on platforms such as Nintendo Wii and  Microsoft Kinect for XBOX users usually take time to get used to the  controls and gaming methods. For users who are taking longer than usual  to learn these controls methods, the system could notify them to move  their arms further by an auditory cue with a higher pitch, in the  scenario of e.g. a golf or bowling game. An immediate auditory cue in  response to the user’s movement has a stronger effect than a purely  visual one.
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>3.2 Auditory versus Visual interfaces while driving
<br>
<br>Two very similar research papers looked into using an auditory interface for while driving (Takao et al., 2002).
<br>
<br>These  user studies investigate two auditory interfaces in a driving  simulator. The same reasoning is used as previously looked at  investigations, where the research is conducted to find alternative ways  of transferring information due to the overloaded visual channel of  humans during driving.
<br>
<br>In  combination with a voice recognition system the driver can perform and  confirm tasks by using his or her voice and ears, such as getting  directions from a GPS system.
<br>
<br>Further research is planned to apply a similar system to public services such as ATM’s and public transport.
<br>
<br>The  second research tries to answer the question whether an auditory user  interface for a mobile phone is less distracting and more efficient than  a visual interface while driving a car (Sodnik et al., 2008). Drivers’  performances were observed while they were driving a simulated vehicle  consisting of a large projection screen, a steering wheel, an  accelerator, a brake and a mobile communication device which could be  controlled by a custom-made interaction device attached to the steering  wheel.
<br>
<br>Three  different experiments were conducted. One visual and two auditory. The  conclusion of this research was that using an auditory interface while  driving improved the safety of the driver significantly.
<br>
<br>
<br>
<br>
<br>
<br>
<br>3.3 Sense of presence
<br>
<br>This  research looked into how multi-sensory input has an effect on the sense  of presence in Virtual Environments (Dinh et al., 1999). The way this  was tested was by putting participants into a virtual reality  environment of an office space with the idea that the participant is a  customer looking to rent an office space. As the participants would  observe the 3D environment, they would be presented with different types  of cues such as odours, ambient sounds, heat and wind in various  combinations. The participants would then afterwards be asked to pick  one of the rooms as a room that they would rent.
<br>
<br>Figure  7 shows results from the user study, comparing the expected results and  the standard deviations of both the Presence rating and the  questionnaire results.
<br>
<br>
<br>
<br>Figure7.PNG
<br>Figure 7. Dinh’s test results (Dinh et al., 1999).
<br>
<br>One of the conclusions of this research was that auditory cues had a large impact on the sense of presence of the participants.
<br>
<br>
<br>
<br>
<br>
<br>4. Virtual Testing Environment Design
<br>For  this investigation a virtual environment will be created using Unity  software (Technologies, 2018). Within the environment three simple  interactive tasks will be created. Applicants of the test are asked to  perform three similar tasks while being instructed in different visual  or auditory ways. The difference in the type of instructions will  determine the major differences in efficiency and usability of auditory  cues in virtual worlds.
<br>
<br>All  three tasks will be kept as similar as possible in room layouts and  type of tasks. The main difference between the tasks will be the way  that the participant is instructed to complete the task.
<br>
<br>The participant uses a mouse and keyboard to control a first-person controller in the environment.
<br>
<br>The  participants will also wear headphones during the tasks to allow them  to experience three-dimensional audio during the auditory focused task.
<br>
<br>
<br>
<br>4.1 The first task
<br>
<br>The purpose of the first task is to instruct the participant with visual cues only.
<br>
<br>In  this task the participant is placed in a room that contains three  coloured keys and three coloured doors. The participant is placed in  front of a table that holds the three keys. The goal of the task is to  bring the three keys to the corresponding doors. Once the participant  starts the task, a timer will start running in the top left of the  screen. The green key will only work on the green door, the red key will  only work on the red door and the blue key only works on the blue door.
<br>
<br>There  are no direct instructions and the participant needs to figure out what  to do in the environment purely based on the information that is  available in the room, which in this task are purely visual. Once the  participant has brought all the keys to the corresponding doors, the  task will end, and the participant is asked to fill out the questions  for task 1.
<br>
<br>4.2 The second task
<br>
<br>In the second task the instructions primarily consist of auditory verbal cues.
<br>
<br>In  this task the colours of the keys have been changed to grey and all  three keys have been shaped with three different shapes. The three  different shapes are a triangle, a circle and a square. All the doors  have also been changed to grey to remove as many visual cues as  possible. Once the user holds one of the keys and carries it to one of  the doors, a spoken message is played that describes the shape of one of  the three keys with the words “triangle”, “circle” or “square”. The  participant must hereby determine which of the cards correspond to which  door. Again, once the participant has brought all the keys to the  corresponding doors, the task will end, and the participant is asked to  fill out the questions for task 2.
<br>
<br>4.3 The third task
<br>
<br>The  third task focuses on using auditory non-verbal cues. Meaning that  instead of using spoken words to describe the goal, non-verbal sounds  are used. The keys and doors in this task have been stripped of both  colours and recognisable shapes. Once the participant picks up a key, a  sound will start playing from the direction of one the doors. The  participant must listen to the sounds carefully to determine from which  direction the sound is coming to be able to figure out which key belongs  to which door. Again, once the participant has brought all the keys to  the corresponding doors, the task will end, and the participant is asked  to fill out the questions for task 3.
<br>
<br>
<br>
<br>
<br>
<br>4.4 Design progression
<br>
<br>During the creation of the tasks the tasks went through multiple design changes.
<br>
<br>During  the creation of the third task the room size was increased so it would  be more obvious from which direction the sounds are coming during that  task. After evaluating the initial task designs it was found that the  inconsistent room sizes between the tasks would deprecate the accuracy  of the test results. This is because the participant would require  different durations to walk from the keys to the doors. The same issues  arose for the way that keys could be picked up. In the first iteration  of the tasks it was possible to pick up multiple keys at the same time.  Due to the fact that every key has a different effect in the second and  third task compared to the first task, it was not possible to pick up  more than one key at the same time. This later had to be changed for the  audio-based tasks. To maintain equality between the tasks and to  maintain accurate test results, the way keys are picked up was changed  to be the same throughout all tasks, that being the same as the second  and third task where the participant is only able to hold one key at the  same time.
<br>
<br>At  the very beginning of the development of the environment, cards were  used instead of keys. To further improve the natural associations  between the participant and the environment, the cards in all the tasks  were changed to different coloured and shaped keys, as shown in Figure  8. The reason for this being the fact that there was no relation between  cards and doors.
<br>
<br>
<br>
<br>Figure8.PNG
<br>Figure 8.
<br>
<br>Further  inconsistencies were found in the use of different objects that would  serve as the target of the participant. In the first task the user had  to place cards in a container, while in the second and third task the  participant had to carry the cards towards a door (Figure 9.). This is  another example of inconsistencies between the tasks and was therefore  changed for all tasks to be doors instead of  containers.
<br>
<br>Figure9.PNG
<br>Figure 9.
<br>
<br>Furthermore,  the sound that represents the location of the correct door in the third  task was replaced with the sound of a doorbell instead of a constant  tone. Initial testing of this task showed that the previous sound used  in this task was more likely to steer a participant away from the  correct door due to the nature of the sound being annoying and  irritating. The sound of a doorbell makes more sense in the context of  the doors within the tasks.
<br>
<br>Overall,  the improvements that were made between the first iteration of the  environment and the final iteration were primarily focused on improving  the interactions and making them more directed towards behavioural and  reflective cognitive processing, and secondarily to ensure the accuracy  and credibility of the results. 
<br>
<br>4.5 Testing
<br>
<br>Testing with participants started on 12-03-2018 and ended on 13-04-2018.
<br>
<br>The  participants would be instructed to first fill out their name, age and  their experience level with computer systems, and then start the tasks.  Emphasize was given that the questionnaire is specifically based around  the interaction of finding out what to do within the environment.
<br>
<br>Between  every task the participant would be asked to fill out a questionnaire  about the usability of the system. The questionnaire consisted of 9  questions.
<br>
<br>No issues arose during the process of the participants performing the tasks.
<br>
<br>Figure10.PNG
<br>Figure 10. The first screen of the questionnaire.
<br>
<br>
<br>
<br>
<br>
<br>4.6 Evaluation of test results
<br>
<br>There  was a total of 13 participants, ranging from age 20 to 25. 8 out of 13  participants considered their experience with computers to be on an  expert level, with the leftover 5 considering their experience with  computers to be intermediate.
<br>
<br>For  each task the average, median and mode was found. It is with that  information that a conclusion about the usability of the systems can be  determined.
<br>
<br>Note:  All the testing data is available in Appendix B. The results are shown  in numbers between 1 and 5 where 1 stands for Strongly Disagree and 5  stands for Strongly Agree. For the duration of how long a participant  took to complete a task, the information is shown in number of seconds.
<br>
<br>
<br>
<br>Question 1: “I found matching the keys with the doors unnecessarily complex.”
<br>
<br>This  question got an average score of 1.6, 2.4 and 3 for each respective  task, which shows a slight increase of complexity of performing the task  per task. Overall the results show that in this case that less visual  cues mean a more complex task.
<br>
<br>
<br>
<br>Question 2: “I thought matching the keys with the doors was an easy task to complete.”
<br>
<br>This  question got an average score of 4.8, 4 and 3.9 for each respective  task. The decreasing numbers show an increase of difficulty per task,  with the largest gap being between the visual task (Task 1) and the  auditory tasks (Task 2 and 3).  Overall the results show that less  visual cues mean a more difficult task to perform.
<br>
<br>
<br>
<br>Question 3: “I feel that I would need the support of a technical person to be able to use this system.”
<br>
<br>This  question got an average score of 1.3, 1.5 and 1.5 for each respective  task, which shows very small increase of a user potentially asking for  technical help when most of previous visual cues have disappeared.  Overall the results show that almost nobody required technical  assistance during the tasks, this is mainly due to the fact that most  participants considered themselves to be well experienced with computer  systems.
<br>
<br>
<br>
<br>Question 4: “I found the various functions during the task were well integrated.”
<br>
<br>This  question got an average score of 4.4, 4.0 and 4.4 for each respective  task, which shows that the verbal auditory task (Task 2) has slightly  worse integrated functionalities compared to the other two tasks.
<br>
<br>Reason  for this can be that in the verbal auditory task (Task 2) the user does  not receive any auditory feedback until the user has walked towards a  door, whereas in the visual task (Task 1) and non-verbal auditory task  (Task 3) the user either has immediate feedback from its starting  position, or when one of the key has been picked up.
<br>
<br>Question 5: “I thought there was too much inconsistency within the task.”
<br>
<br>This  question got an average score of 1.2, 1.7 and 1.4 for each respective  task, which shows that the verbal auditory task (Task 2) stands out as  being the least consistent. Overall all tasks are considered to be  consistent due to all average results ultimately disagree with the  question. 
<br>
<br>
<br>
<br>Question 6: “I would imagine that most people would learn to solve this task very quickly.”
<br>
<br>This  question got an average score of 4.1, 4 and 3.6 for each respective  task, which shows a slight decrease of participants who believed most  people would solve the tasks quickly, per task. This shows that  unrelated to how well the participant solved the task him or herself, he  or she believes it is generally easy to solve.
<br>
<br>
<br>Question 7: “I found matching the keys with the doors very cumbersome to perform.”
<br>
<br>This  question got an average score of 1.2, 1.3 and 1.9 for each respective  task, which shows a very small increase of participants believing the  tasks are difficult to perform, per task. Overall the tasks were not  experienced as cumbersome.
<br>
<br>For  this task a lot of the participants required the word “Cumbersome” to  be defined to them. With the definition of the word being “large or  heavy and therefore difficult to use” most participants choose to say  the tasks were not cumbersome to perform.
<br>
<br>
<br>
<br>Question 8: “I felt very confident matching the keys with the doors.”
<br>
<br>This  question got an average score of 4.6, 3.7 and 4 for each respective  task, which shows that participants were least confident solving the  verbal auditory task (Task 2), and most confident solving the visual  task (Task 1).
<br>
<br>
<br>
<br>Question 9: “I needed to learn a lot of things before I could get going with this system.”
<br>
<br>This  question got an average score of 1.6, 2 and 2.2 for each respective  task, which shows a slight increase of participants feeling like they  had to learn a lot before being able to start using a task, per task.  The visual task (Task 1) scores as the easiest task, this directly  relates to the explanation of instructional feedback at question 4.
<br>
<br>Overall the results show that participants found it easy to get going with the tasks.
<br>
<br>
<br>
<br>
<br>Completion times:
<br>
<br>The  average completion times per task were 22 seconds for the first task,  25 seconds for the second task and 30 seconds for the third task. The  increase of duration directly relates to the difference in instructional  types, and also relates to what the results of the questionnaire tell  us regarding to the difficulty of the tasks.
<br>
<br>There  is an average 3 second increase between the visual (Task 1) and verbal  auditory task (Task 2) and a 5 second increase between the verbal  auditory task (Task 2) and the non-verbal auditory task (Task 3).  Expected results were that the highest time different of completion time  would be between the visual and verbal auditory task due to the  participant having to discover that the instruction type had changed  from visual to auditory instructions, yet the larger difference in  completion time was between the verbal auditory and non-verbal auditory  task.
<br>
<br>
<br>
<br>4.7 Summary
<br>
<br>Testing  has shown that the main difference between the effectiveness of visual  cues and auditory cues in virtual worlds are that one performs better  than the other, without the other. Instead of each task incrementally  becoming more difficult and less usable, there was an unexpected result  that showed the least performing task to be the verbal-auditory task  instead of the non-verbal auditory task, yet it took most participants  the longest to complete the non-verbal auditory task. There is a high  probability that this is related to the design of the verbal auditory  task which could have been improved to be more consistent. Taking this  into consideration, there was an obvious but not very large difference  between participants performances between the type of tasks.
<br>
<br>What  is clear is the difference between the visual tasks and the auditory  tasks, where performance was better with visual task without auditory  cues, compared to auditory tasks with less visual cues.
<br>
<br>
<br>
<br>
<br>4.7.1 Difference in participants
<br>
<br>experience with computer systems
<br>
<br>The  participants’ prior knowledge of computer systems was primarily  considered due to the tasks having to be controlled through a mouse and  keyboard setup using WASD-controls.
<br>
<br>By  separating the results, we can see that all tasks would be performed in  a much shorter duration by participants who considered themselves  expect computer users compared to the average of all intermediate users.  For the first task there is a 5 second difference. For the second task  there is a 3 second difference. For the third task there is a 15 second  difference.
<br>
<br>This in itself shows that it is important to consider the characteristics of your participants while performing user studies.
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>5. Conclusion
<br>Albeit  the auditory sense not being the primary sense for humans to interact  with their surroundings, auditory cues can be extremely useful in  certain scenarios. With humans relying for 80% on visual cues, it is in  the leftover 20% where design choices can also be considered to convey  information and/or instruct a user efficiently, specifically in 3D  spaces. Which content is best to use for the auditory cues directly  depends on the content of the virtual environment and depends on which  content the auditory cue relates to. When the purpose of the auditory  cue is to inform the user of a location of something that is offscreen,  the auditory cue must be directly on top of the object or location and  it must not change location for the user to be able to find the exact  spot of the auditory cue within the world. If an auditory cue wants to  warn the user for something that is offscreen as fast as possible, it  should be done in combination with clear visual cues.
<br>
<br>It  is safe to conclude that auditory cues are primarily useful to convey  information such as a location within a 3D space, and to reduce the  cognitive load of the visual sense if there is a risk of overloading it.
<br>
<br>When  deciding on which sound to use it helps to use sounds that relate to  the object it is referring to. In a virtual world, any type of sound  will grab the user’s attention to the location of an object, but when  using the wrong type of sound, it might not have the desired results.  The user might misinterpret the sound as something that is supposed to  lead him or her away from the object. The type of sound that is chosen  needs to make sense in relation to the object, e.g. in the testing  environment for this investigation, the monotone pitch used on the doors  in an early version of the testing environment would drive participants  away from the door where the sound was coming from, while using the  sound of a doorbell would lead them towards it  door.
<br>
<br>5.1 Reflection And Expanding The Investigation
<br>
<br>The  major change that should be considered for the design of the virtual  testing environment lies with the second task, the verbal auditory task.  Instead of the names of the shapes being pronounced near the doors, it  should be the other way around where possibly the shapes of the doors  had changed instead of the shapes of the keys. The shapes of the doors  would then be pronounced to the user once he or she picks up one of the  keys. This way the user received immediate feedback about where to place  the keys instead of having to walk towards the doors to receive  feedback. This difference showed to make a difference in completion time  in the test results compared to other tasks.
<br>
<br>Further  testing can still be carried out by creating more types of tasks for  the participants to carry out. For example, tasks that would include  asynchronous distractions within the virtual environment and different  ways of interacting with audio, such as dancing, playing an instrument  and using voice commands.
<br>
<br>Furthermore,  testing could have included different types of audio and different  environment with audio noise, in which the conspicuousness of certain  auditory scenarios could be tested.
<br>
<br>Another  testing environment was considered for this investigation. To fully  immerse a testing participant into a 3D environment, a virtual reality  environment can be created. By removing the mouse and keyboard controls  of the standard computer controls, and the use of as head, hand and leg  tracking sensors would allow the participants to interact with the  virtual world on a much more natural level. This would also allow for  auditory cues in a Virtual Environment to be tested more accurately.
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>6. References
<br>Ad Alternum (2018) OrbusVR. Available at: https://orbusvr.com.
<br>
<br>Argyle, M. (1972) ‘Non-verbal communication in human social interaction.’, in Non-verbal communication. Oxford,  England: Cambridge U. Press, p. 443.
<br>
<br>Bahrick,  L. E., Lickliter, R. and Flom, R. (2004) ‘Intersensory Redundancy  Guides the Development of Selective Attention, Perception, and Cognition  in Infancy’.
<br>
<br>Bailenson, J. N. et al. (2002) ‘Interpersonal Distance in Immersive Virtual Environments’. doi: 10.1177/0146167203253270.
<br>
<br>Baldis, J. J. (2001) ‘Effects of Spatial Audio on Memory, Comprehension, and Preference during Desktop Conferences’.
<br>
<br>Brown, J. R. et al. (1999) ‘Human-centered computing, online communities, and virtual environments’, IEEE Computer Graphics and Applications. doi: 10.1109/38.799742.
<br>
<br>CipSoft GmbH (2018) Tibia. Available at: http://www.tibia.com.
<br>
<br>Dalgarno, B. and Lee, M. J. W. (2010) ‘What are the learning affordances of 3-D virtual environments?’, British Journal of Educational Technology. doi: 10.1111/j.1467-8535.2009.01038.x.
<br>
<br>Dinh,  H. Q. et al. (1999) ‘Evaluating the importance of multi-sensory input  on memory and the sense of presence in virtual environments’, in Proceedings IEEE Virtual Reality (Cat. No. 99CB36316). doi: 10.1109/VR.1999.756955.
<br>
<br>Fetscherin, M. and Lattemann, C. (2008) ‘USER ACCEPTANCE OF VIRTUAL WORLDS’, Journal of Electronic Commerce Research, 9(3).
<br>
<br>Gaver, W. W. (1986) ‘Auditory Icons: Using Sound in Computer Interfaces’, Human–Computer Interaction. Taylor &amp; Francis, 2(2), pp. 167–177. doi: 10.1207/s15327051hci0202_3.
<br>
<br>Google LLC (2018a) Google Cardboard. Available at: https://vr.google.com/cardboard.
<br>
<br>Google LLC (2018b) Google Daydream. Available at: https://vr.google.com/daydream.
<br>
<br>Holmes, O. W. (1859) ‘The Stereoscope and the Stereograph’, The Atlantic Monthly, 3(June), pp. 738–748.
<br>
<br>HTC Corporation (2018) HTC Vive. Available at: https://www.vive.com.
<br>
<br>Jeong,  D. and Yi, W. (2015) ‘Applications of Auditory Cues for Spatial  Cognitive Behaviors Based on Embodied Music Cognition’, (2010), pp.  58–62.
<br>
<br>Kahneman, D. (2010) Thinking, Fast and Slow.
<br>
<br>Microsoft (2018) HoloLens. Available at: https://www.microsoft.com/en-gb/hololens.
<br>
<br>NCSOFT Corp. (2018) ‘Aion’. Available at: http://na.aiononline.com/en.
<br>
<br>Norman, D. (2013) The Design of Everyday Things.
<br>
<br>Oculus VR, L. (2018) Oculus Rift. Available at: https://www.oculus.com.
<br>
<br>Oxford Dictionary (2018). Available at: https://en.oxforddictionaries.com.
<br>
<br>Rohrer, T. (2007) ‘The Body in Space; Dimensions of Embodiment’.
<br>
<br>Samsung Electronics (2018) ‘Gear VR’. Available at: http://www.samsung.com/global/galaxy/gear-vr.
<br>
<br>Sauro, J. (2011) System Usability Scale. Available at: https://measuringu.com/sus/ (Accessed: 13 November 2017).
<br>
<br>Sharp, H., Preece, J. and Rogers, Y. (2015) ‘Interaction Design: Beyond human-computer interaction’, pp. 67–71.
<br>
<br>Shin,  D.-H. (2017) ‘The Role of Affordance in the Experience of Virtual  Reality Learning: Technological and Affective Affordances in Virtual  Reality’, Telematics and Informatics. doi: 10.1016/j.tele.2017.05.013.
<br>
<br>Smith, J. (2016) THE VIRTUAL REALITY REPORT: How the early days of VR are unfolding and the challenges it must overcome to reach mass adoption. Available at: http://uk.businessinsider.com/virtual-reality-report-2016-11 (Accessed: 20 November 2017).
<br>
<br>Sodnik, J. et al. (2008) ‘A user study of auditory versus visual interfaces for use while driving’, International Journal of Human Computer Studies. doi: 10.1016/j.ijhcs.2007.11.001.
<br>
<br>Sony Interactive Entertainment (2018) ‘Playstation VR’. Available at: https://www.playstation.com/en-gb/explore/playstation-vr.
<br>
<br>Steenson,  C. J. and Rodger, M. W. M. (2015) ‘Bringing Sounds into Use: Thinking  of Sounds as Materials and a Sketch of Auditory Affordances’, The Open Psychology Journal, 8(3), pp. 2–174.
<br>
<br>Takao, H. et al. (2002) ‘Acoustic user interface (AUI) for the auditory displays’, Displays. doi: 10.1016/S0141-9382(02)00011-2.
<br>
<br>Unity Technologies (2018) Unity. Available at: https://unity3d.com.
<br>
<br>Valve Corporation (2018) Steam. Available at: steampowered.com.
<br>
<br>VRChat Inc. (2018) VRChat. Available at: https://www.vrchat.net.
<br>
<br>Worldwide virtual reality (VR) headset unit sales by brand in 2016 and 2017 (in millions) (2018). Available at: https://www.statista.com/statistics/752110/global-vr-headset-sales-by-brand/ (Accessed: 20 November 2017).
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>
<br>7. Appendices
<br>Appendix A – System Usability Scale Questionnaire
<br>
<br>Appendix A.png
<br>Appendix B – Full Questionnaire data
<br>
<br>Appendix B1.png
<br>Appendix B2.png</p>
  </div>
  <style data-mode="XL">@media (min-width: 1200px) {
  .u-block-0214-2 {
    min-height: 26820px;
  }
  .u-block-0214-5 {
    line-height: 1.8;
    margin-top: 60px;
    margin-left: 0;
    margin-right: 0;
    margin-bottom: 58px;
  }
}</style>
  <style data-mode="LG">@media (max-width: 1199px) and (min-width: 992px) {
  .u-block-0214-2 {
    min-height: 26820px;
  }
  .u-block-0214-5 {
    line-height: 1.8;
    margin-top: 60px;
    margin-left: 0;
    margin-right: 0;
    margin-bottom: 58px;
  }
}</style>
  <style data-mode="MD">@media (max-width: 991px) and (min-width: 768px) {
  .u-block-0214-2 {
    min-height: 26820px;
  }
  .u-block-0214-5 {
    line-height: 1.8;
    margin-top: 60px;
    margin-left: 0;
    margin-right: 0;
    margin-bottom: 58px;
  }
}</style>
  <style data-mode="SM">@media (max-width: 767px) and (min-width: 576px) {
  .u-block-0214-2 {
    min-height: 26820px;
  }
  .u-block-0214-5 {
    line-height: 1.8;
    margin-top: 60px;
    margin-left: 0;
    margin-right: 0;
    margin-bottom: 58px;
  }
}</style>
  <style data-mode="XS">@media (max-width: 575px) {
  .u-block-0214-2 {
    min-height: 26820px;
  }
  .u-block-0214-5 {
    line-height: 1.8;
    margin-top: 60px;
    margin-left: 0;
    margin-right: 0;
    margin-bottom: 58px;
  }
}</style>
</section><style class="darkreader darkreader--sync" media="screen"></style><style class="darkreader darkreader--sync" media="screen"></style><style class="darkreader darkreader--sync" media="screen"></style><style class="darkreader darkreader--sync" media="screen"></style><style class="darkreader darkreader--sync" media="screen"></style><style class="darkreader darkreader--sync" media="screen"></style><style class="darkreader darkreader--sync" media="screen"></style><style class="darkreader darkreader--sync" media="screen"></style><style class="darkreader darkreader--sync" media="screen"></style><style class="darkreader darkreader--sync" media="screen"></style><style class="darkreader darkreader--sync" media="screen"></style><style class="darkreader darkreader--sync" media="screen"></style><style class="darkreader darkreader--sync" media="screen"></style><style class="darkreader darkreader--sync" media="screen"></style><style class="darkreader darkreader--sync" media="screen"></style><style class="darkreader darkreader--sync" media="screen"></style><style class="darkreader darkreader--sync" media="screen"></style><style class="darkreader darkreader--sync" media="screen"></style><style class="darkreader darkreader--sync" media="screen"></style><style class="darkreader darkreader--sync" media="screen"></style><style class="darkreader darkreader--sync" media="screen"></style>
<style class="darkreader darkreader--sync" media="screen"></style>
<style class="darkreader darkreader--sync" media="screen"></style>
<style class="darkreader darkreader--sync" media="screen"></style>
<style class="darkreader darkreader--sync" media="screen"></style>
</body></html>